{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f32ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os; os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import sys\n",
    "sys.path.append(\"../../../../cds_vision_tools\")\n",
    "sys.path.append(\"../../../../cds_vision_tools/cds_vision_tools/pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e99db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "import time\n",
    "from PIL import Image\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22f711",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from segmentation.annotator import coco2mask, read_coco_annotation_json_file, load_and_display_image_with_mask, add_background_images, get_image_mask_tensor\n",
    "from segmentation.dataset import SegmentationDataset\n",
    "from segmentation.model import LoadSegmentationModel\n",
    "from segmentation.trainer import SegmentationTrainer\n",
    "from segmentation.inference import img_inference, display_results\n",
    "from vision_utils import preprocess_image, save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c61a9",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb5134",
   "metadata": {},
   "source": [
    "### Data Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c6b4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "seg_data_dir = Path(\"../../../test/cds_vision_tools/pytorch/segmentation/TestProject\")\n",
    "# Annotation file\n",
    "path_to_annotation = seg_data_dir / \"annotations-unittest.json\"\n",
    "\n",
    "# images directory\n",
    "original_images_dir = seg_data_dir / \"images\"\n",
    "background_images_dir = seg_data_dir / \"background\"\n",
    "augmented_images_dir = seg_data_dir / \"augmented_images_path\"\n",
    "\n",
    "# masks directory\n",
    "masks_dir = seg_data_dir / \"masks\"\n",
    "train_masks_dir = masks_dir / \"train\"\n",
    "val_masks_dir = masks_dir / \"val\"\n",
    "\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6c458c",
   "metadata": {},
   "source": [
    "### Create augmented images and masks from the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c59bee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "coco2mask([path_to_annotation],\n",
    "            original_images_dir,\n",
    "            masks_dir,\n",
    "            augmented_images_dir,\n",
    "            background_images_dir,\n",
    "            train_portion=0.7,\n",
    "            val_portion=0.2,\n",
    "            shuffle=True,\n",
    "            augmentation_portion=1.0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73234f",
   "metadata": {},
   "source": [
    "### Display image and mask of a sample image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e6f7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1\n",
    "image_file = \"../../../test/cds_vision_tools/pytorch/segmentation/TestProject/augmented_images_path/10-29-22-66_20231205-190806-6_augmented.jpg\"\n",
    "mask_file = \"../../../test/cds_vision_tools/pytorch/segmentation/TestProject/masks/train/10-29-22-66_20231205-190806-6_augmented.pt\"\n",
    "load_and_display_image_with_mask(image_file, mask_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf5889",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2\n",
    "annotations_df = read_coco_annotation_json_file(Path(path_to_annotation))\n",
    "annotations_df = add_background_images(background_images_dir, annotations_df)\n",
    "display(annotations_df)\n",
    "\n",
    "# Check the mask of an image with annotation\n",
    "img_t, mask_t = get_image_mask_tensor(original_images_dir, background_images_dir, annotations_df.iloc[0][\"file_name\"], annotations_df.iloc[0][\"segmentation\"])\n",
    "mask_array = mask_t.squeeze().numpy()\n",
    "plt.imshow(mask_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66070d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the mask is empty for images with only background\n",
    "img_t, mask_t = get_image_mask_tensor(original_images_dir, background_images_dir, annotations_df.iloc[12][\"file_name\"], annotations_df.iloc[12][\"segmentation\"])\n",
    "mask_array = mask_t.squeeze().numpy()\n",
    "plt.imshow(mask_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fc602",
   "metadata": {},
   "source": [
    "### Generate Training and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9635166",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation\n",
    "train_dataset = SegmentationDataset(\n",
    "    [original_images_dir, augmented_images_dir], train_masks_dir, os.listdir(train_masks_dir)\n",
    ")\n",
    "val_dataset = SegmentationDataset(\n",
    "    [original_images_dir, augmented_images_dir], val_masks_dir, os.listdir(val_masks_dir)\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51226163",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9514e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the default segmentation model i.e deeplabv3 with MobileNet\n",
    "model_loader = LoadSegmentationModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d1a3d",
   "metadata": {},
   "source": [
    "### Model Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72375d6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "loss_fun = torch.nn.CrossEntropyLoss()\n",
    "LR = 0.001\n",
    "optimizer = torch.optim.Adam(model_loader.model.parameters(), lr=LR)\n",
    "\n",
    "model_dir = seg_data_dir / \"models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f4758",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc0c1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SegmentationTrainer(\n",
    "            model_loader.model,\n",
    "            train_data_loader,\n",
    "            val_data_loader,\n",
    "            optimizer,\n",
    "            model_dir,\n",
    "            loss_fun,\n",
    "            num_epochs=epochs,\n",
    "        )\n",
    "# Train the model and capture the returned metrics dictionary\n",
    "metrics_dict = trainer.train()\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f28c27",
   "metadata": {},
   "source": [
    "# Load trained Segmentation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074e40a",
   "metadata": {},
   "source": [
    "### Saved Model Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47030aa7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = seg_data_dir / \"models\"\n",
    "model_path = model_dir / \"../models/wreckathon_seg_epoch_10.pt\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49356fda",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6ee0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# wreckathon_seg_epoch_10 is deeplabv3_mobilenet model\n",
    "model_loader = LoadSegmentationModel()\n",
    "\n",
    "# Load model state dictionary\n",
    "model_loader.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_loader.model.eval()\n",
    "model_loader.model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e6f81",
   "metadata": {},
   "source": [
    "# Inference on Single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d31a0",
   "metadata": {},
   "source": [
    "### Image Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859afe28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_file = \"10-29-22-66_20231205-200437-10.jpeg\"\n",
    "image_file_path = original_images_dir / image_file\n",
    "\n",
    "# Read image with OpenCV\n",
    "img = cv2.imread(str(image_file_path))\n",
    "hotspot = {\"top\": 100, \"bottom\": 415, \"left\": 500, \"right\": 900}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3454a3",
   "metadata": {},
   "source": [
    "### Run Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa070cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# start time\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Preprocess the image\n",
    "resized_img, processed_img = preprocess_image(img, hotspot)\n",
    "\n",
    "# Inference\n",
    "overlay_img = img_inference(model_loader.model, resized_img, processed_img)\n",
    "\n",
    "# end time\n",
    "inf_time = time.perf_counter()-start\n",
    "print(f'Cost {inf_time} s')\n",
    "\n",
    "# plot the images\n",
    "display_results(resized_img, overlay_img)\n",
    "\n",
    "# Save overlay image\n",
    "overlay_img_file = \"overlay-\" + image_file\n",
    "save_image(overlay_img, original_images_dir / overlay_img_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c132972a",
   "metadata": {},
   "source": [
    "# DataLoader for a batchsize of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e41d2e",
   "metadata": {},
   "source": [
    "### DataLoader Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2749098",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "image_display_limit = 3\n",
    "hotspot = {\"top\": 100, \"bottom\": 250, \"left\": 180, \"right\": 350}\n",
    "# hotspot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6131823",
   "metadata": {},
   "source": [
    "### Run Inference\n",
    "Note: Model results shown in the displayed images are not great as the model is trained on the cropped hotspot images but currently run on the entire frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5039733",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize counter for number of images to display\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    # Loop through batches of data within the data loader\n",
    "    for x_batch, y_true_batch in iter(val_data_loader):\n",
    "        for x_img in x_batch:\n",
    "\n",
    "            # convert image from [channels, height, width] to [height, width, channels]\n",
    "            img = np.array(x_img.permute(1, 2, 0).type(torch.int))\n",
    "\n",
    "            # Preprocess the image based on hotspot information\n",
    "            resized_img, processed_img = preprocess_image(img, hotspot)\n",
    "\n",
    "            # Inference\n",
    "            overlay_img = img_inference(model_loader.model, resized_img, processed_img)\n",
    "    \n",
    "            # Display the results\n",
    "            display_results(resized_img, overlay_img)\n",
    "\n",
    "            # Stop processing after certain iterations\n",
    "            i+=1\n",
    "            if i == image_display_limit:\n",
    "                break\n",
    "                \n",
    "        # Exit outer loop as well\n",
    "        if i == image_display_limit:\n",
    "          break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5aefa8",
   "metadata": {},
   "source": [
    "# Inference on whole frame \n",
    "Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b09e8",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7ccc0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# video to process\n",
    "video_dir = seg_data_dir / \"videos\"\n",
    "video_file = \"video2_crop.mp4\" \n",
    "video_path = video_dir / video_file\n",
    "\n",
    "# path to save the video with overlay \n",
    "output_video_file = \"video2_crop_whole_frame.avi\"\n",
    "output_video_path = video_dir / output_video_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71376f07",
   "metadata": {},
   "source": [
    "### Video Capture and Output Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fbd3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Open the video capture object using the video path\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "# Check if video capture was successful\n",
    "if cap.isOpened() == False:\n",
    "    logger.info(\"Error opening video stream or file\")\n",
    "    cap.release()\n",
    "\n",
    "# Get the video frame width and height\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the output video writer object\n",
    "output = cv2.VideoWriter(str(output_video_path),\n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         cap.get(5) * 0.25,\n",
    "                         (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1c284",
   "metadata": {},
   "source": [
    "### Run Inference\n",
    "Note: Model results shown in the displayed images are not great as the model is trained on the cropped hotspot images but currently run on the entire frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e297ab5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Main video processing loop\n",
    "while True:\n",
    "    # Read a frame from the video capture object\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if frame reading was successful (end of video or error)\n",
    "    if not ret:\n",
    "        break  # Exit the loop if frame reading fails\n",
    "\n",
    "    # Preprocess the frame for model inference\n",
    "    resized_img, processed_img = preprocess_image(frame)\n",
    "\n",
    "    # Perform inference on the preprocessed frame using the loaded model\n",
    "    overlay_img = img_inference(model_loader.model, resized_img, processed_img)\n",
    "    \n",
    "    # Resize the overlay image to match the original frame size for proper overlay\n",
    "    masked_image = cv2.resize(overlay_img, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "    plt.imshow(masked_image)\n",
    "    \n",
    "    # Write the processed frame (with segmentation mask) to the output video\n",
    "    output.write(masked_image)  \n",
    "\n",
    "# Release resources after the loop exits\n",
    "cap.release()  # Release video capture object\n",
    "output.release()  # Release video writer object\n",
    "cv2.destroyAllWindows()   # Close any OpenCV windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30716c66",
   "metadata": {},
   "source": [
    "# Inference on a hotspot \n",
    "within the whole frame of the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794813bd",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d10630",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# video to process\n",
    "video_dir = seg_data_dir / \"videos\"\n",
    "video_file = \"video2_crop.mp4\" \n",
    "video_path = video_dir / video_file\n",
    "\n",
    "# path to save the video with overlay \n",
    "output_video_file = \"video2_crop_hotspot.avi\"\n",
    "output_video_path = video_dir / output_video_file\n",
    "\n",
    "# Define hotspot area\n",
    "hotspot = {\"top\": 5, \"bottom\": 320, \"left\": 500, \"right\": 900}\n",
    "entropy_list = deque(maxlen=1000) \n",
    "entropy_change_threshold=2\n",
    "prev_entropy_change_percent = 0\n",
    "event_number = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccaedbf",
   "metadata": {},
   "source": [
    "### Video Capture and Output Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd1865",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Open the video capture object using the video path\n",
    "cap = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "# Check if video capture was successful\n",
    "if cap.isOpened() == False:\n",
    "    logger.info(\"Error opening video stream or file\")\n",
    "    cap.release()\n",
    "\n",
    "# Get the video frame width and height\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the output video writer object\n",
    "output = cv2.VideoWriter(str(output_video_path),\n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         cap.get(5) * 0.25,\n",
    "                         (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fcc45",
   "metadata": {},
   "source": [
    "### Run Inference and count the number of bones passed through the hotspot area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab9a7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Main video processing loop\n",
    "while True:\n",
    "    # Read a frame from the video capture object\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if frame reading was successful (end of video or error)\n",
    "    if not ret:    \n",
    "        break  # Exit the loop if frame reading fails\n",
    "\n",
    "    # Calculate entropy of the hotspot area\n",
    "    entropy, hot_spot_img = get_entropy(frame, hotspot)\n",
    "    \n",
    "    # Update entropy tracking and calculate entropy change percent\n",
    "    entropy_list, entropy_change_percent = entropy_calculator(entropy, entropy_list)\n",
    "\n",
    "    # Determine event status based on entropy change\n",
    "    event_status = get_event_status(entropy_change_percent,\n",
    "                                        prev_entropy_change_percent,\n",
    "                                        entropy_change_threshold)\n",
    "\n",
    "    # Image collection\n",
    "    if event_status == \"image_collection\":\n",
    "        # Preprocess the frame for model inference\n",
    "        resized_img, processed_img = preprocess_image(frame, hotspot)\n",
    "    \n",
    "        # Perform inference on the preprocessed frame using the loaded model\n",
    "        overlay_img = img_inference(model_loader.model, resized_img, processed_img)\n",
    "\n",
    "        # Resize the overlay image to match the hotspot region for proper masking\n",
    "        masked_image = cv2.resize(overlay_img, (hot_spot_img.shape[1], hot_spot_img.shape[0]), interpolation=cv2.INTER_AREA)\n",
    "        # Convert masked image to RGB format for display\n",
    "        masked_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply mask to the original frame within the hotspot region\n",
    "        frame[hotspot[\"top\"]:hotspot[\"bottom\"],\n",
    "              hotspot[\"left\"]:hotspot[\"right\"],\n",
    "              :] = masked_image\n",
    "\n",
    "    # Increment event counter for tracking\n",
    "    elif event_status == \"event_end\":\n",
    "        event_number += 1\n",
    "\n",
    "    # Update tracking variables and display frame\n",
    "    prev_entropy_change_percent = entropy_change_percent\n",
    "\n",
    "    # Draw a rectangle around the hotspot region on the frame\n",
    "    frame = cv2.rectangle(\n",
    "            frame,\n",
    "            (hotspot[\"left\"], hotspot[\"top\"]),\n",
    "            (hotspot[\"right\"], hotspot[\"bottom\"]),\n",
    "            (255, 0, 0),\n",
    "            1,\n",
    "        )\n",
    "\n",
    "    # Add text overlay on the frame to display the current event count\n",
    "    frame = cv2.putText(\n",
    "            frame,\n",
    "            f\"Events : {event_number}\",\n",
    "            (100, 100),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 0, 0),\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    plt.imshow(frame)\n",
    "    # Write the processed frame (with segmentation mask) to the output video\n",
    "    output.write(frame)\n",
    "\n",
    "# Release resources after the loop exits\n",
    "cap.release()  # Release video capture object\n",
    "output.release()  # Release video writer object\n",
    "cv2.destroyAllWindows()   # Close any OpenCV windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85733fe",
   "metadata": {},
   "source": [
    "# Inference on a whole frame \n",
    "IP address of the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d57b6",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaed8a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Image capture settings\n",
    "images_per_cycle = 400\n",
    "cam_id = 63\n",
    "stream_protocol = \"rtsp\"\n",
    "\n",
    "# connect to ip camera\n",
    "ip_address = f\"10.29.22.{cam_id}/profile2/media.smp\"\n",
    "# AXIS camera - f\"10.29.22.{cam_id}/axis-media/media.amp\"\n",
    "video_address = f\"{stream_protocol}://{username}:{password}@{ip_address}\"\n",
    "\n",
    "# path to save the video from IP address with overlay \n",
    "output_video_file = f\"ip_{cam_id}.avi\"\n",
    "output_video_path = video_dir / output_video_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f145a57",
   "metadata": {},
   "source": [
    "### Video Capture and Output Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f386c3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Open the video capture object using the IP address\n",
    "cap = cv2.VideoCapture(video_address)\n",
    "\n",
    "# Check if video capture was successful\n",
    "if not cap.isOpened():\n",
    "    print(f\"Cannot open RTSP stream for camera - {cam_id}\")\n",
    "    cap.release()\n",
    "\n",
    "# fps of the camera\n",
    "input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_rate = 0.25\n",
    "output_fps = input_fps * frame_rate\n",
    "\n",
    "# Get the video frame width and height\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the output video writer object\n",
    "output = cv2.VideoWriter(str(output_video_path),\n",
    "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                         output_fps,\n",
    "                         (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3b9b16",
   "metadata": {},
   "source": [
    "### Run Inference on limited frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933cc96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "\n",
    "# Main video processing loop\n",
    "while True & (frame_count < images_per_cycle):    \n",
    "    # Read a frame from the video capture object\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if frame reading was successful (end of video or error)\n",
    "    if not ret:\n",
    "        break  # Exit the loop if frame reading fails\n",
    "\n",
    "    # Preprocess the frame for model inference\n",
    "    resized_img, processed_img = preprocess_image(frame)\n",
    "\n",
    "    # Perform inference on the preprocessed frame using the loaded model\n",
    "    overlay_img = img_inference(model_loader.model, resized_img, processed_img)\n",
    "\n",
    "    # Resize the overlay image to match the original frame size for proper overlay\n",
    "    masked_image = cv2.resize(overlay_img, (np.shape(frame)[1],np.shape(frame)[0]), interpolation=cv2.INTER_AREA)\n",
    "    # Convert masked image to RGB format for display\n",
    "    masked_image = cv2.cvtColor(masked_image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(masked_image)\n",
    "\n",
    "    # increase counter\n",
    "    frame_count += 1\n",
    "\n",
    "    # Write the processed frame (with segmentation mask) to the output video\n",
    "    output.write(masked_image)\n",
    "\n",
    "# Release resources after the loop exits\n",
    "cap.release()  # Release video capture object\n",
    "output.release()  # Release video writer object\n",
    "cv2.destroyAllWindows()   # Close any OpenCV windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d3dbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
