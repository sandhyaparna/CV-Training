{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af3fac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014de8e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\", \"../../src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f453926",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from model_inference import compare_two_models, visualize_detections, display_text_block, visualize_and_save, infer_image, infer_fp16_image, visualize_fp16\n",
    "from compare_videos import play_top_bottom, play_side_by_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d20e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MODELS - https://github.com/pytorch/vision/tree/main/torchvision/models/detection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    ssd300_vgg16, SSD300_VGG16_Weights,\n",
    "    ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff8e1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2  # background + 1 class (adjust if needed)\n",
    "label_map = {1: 'object'}  # your label map\n",
    "score_thr = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d52c8e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "v1_model_path = Path(\"../models/v1/v1_rcnn_resnet50.pth\")\n",
    "print(f\"\\033[91m{v1_model_path}\\033[0m\")\n",
    "\n",
    "v1_model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "# get number of input features for the classifier\n",
    "in_features = v1_model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "v1_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "v1_model.load_state_dict(torch.load(v1_model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "v1_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9134e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "v2_model_path = Path(\"../models/v2/v2_rcnn_resnet50.pth\")\n",
    "print(f\"\\033[91m{v2_model_path}\\033[0m\")\n",
    "\n",
    "v2_model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "# get number of input features for the classifier\n",
    "in_features = v2_model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "v2_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "v2_model.load_state_dict(torch.load(v2_model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "v2_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8c32c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "kd_model_path = Path(\"../models/distilled/kd_resize_rot_frcnn_mobilenet_epoch8.pth\")\n",
    "print(f\"\\033[91m{kd_model_path}\\033[0m\")\n",
    "\n",
    "kd_model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "in_feat = kd_model.roi_heads.box_predictor.cls_score.in_features\n",
    "kd_model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "\n",
    "kd_model.load_state_dict(torch.load(kd_model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "kd_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3a9f5",
   "metadata": {},
   "source": [
    "### v1 vs v2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4f8104",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Run on all test images\n",
    "# -----------------------\n",
    "test_dir = \"../data/test_images\"\n",
    "for fname in os.listdir(test_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")):\n",
    "        continue\n",
    "    img_path = os.path.join(test_dir, fname)\n",
    "    out_path = compare_two_models(img_path, v1_model, \"v1_model\", v2_model, \"v2_model\", label_map, score_thr)\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c355ab",
   "metadata": {},
   "source": [
    "### v2 vs KD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4503e32a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Run on all test images\n",
    "# -----------------------\n",
    "test_dir = \"../data/test_images\"\n",
    "for fname in os.listdir(test_dir):\n",
    "    if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\")):\n",
    "        continue\n",
    "    img_path = os.path.join(test_dir, fname)\n",
    "    out_path = compare_two_models(img_path, v2_model, \"v2_model\", kd_model, \"kd_model\", label_map, score_thr)\n",
    "    print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c06142",
   "metadata": {},
   "source": [
    "# Qunantized Int8 Model\n",
    "scripted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32568812",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "save_dir = Path(\"../models/frcnn_mobilenet_quantized\")\n",
    "scripted_path = save_dir / \"model_scripted_quant.pt\"   # produced by torch.jit.save(...)\n",
    "test_images_dir = Path(\"../data/test_images\")          # <-- change to your test folder\n",
    "out_dir = Path(\"../outputs/quant_infer\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "score_thresh = 0.5   # tweak as needed\n",
    "max_dets     = None  # e.g., 50\n",
    "\n",
    "# Device: quantized models run on CPU\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13694724",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Pick the CPU int8 backend:\n",
    "# - 'qnnpack' works everywhere and is best for ARM/mac/x86 generally\n",
    "# - 'fbgemm' is best for x86 AVX2/AVX512 servers\n",
    "torch.backends.quantized.engine = \"qnnpack\"  # or \"fbgemm\"\n",
    "\n",
    "# 2) Load on CPU (quantized models must run on CPU)\n",
    "quant_model = torch.jit.load(scripted_path, map_location=device)\n",
    "quant_model.eval()\n",
    "\n",
    "# Sanity: make sure the model is on CPU\n",
    "for n, m in quant_model.named_modules():\n",
    "    if hasattr(m, \"weight\"):\n",
    "        try:\n",
    "            _ = m.weight.device\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"Loaded quantized TorchScript model on CPU with engine:\", torch.backends.quantized.engine)\n",
    "\n",
    "# Preprocessing: Faster R-CNN expects a FloatTensor [0,1], CxHxW\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "# Optional: class map (index -> name). Edit to your dataset.\n",
    "# 0 is background by convention; your training 'num_classes' included it.\n",
    "id2name = {\n",
    "    1: \"object\",  # <-- replace with your class names\n",
    "    # 2: \"another_class\",\n",
    "}\n",
    "\n",
    "# Simple inference helper\n",
    "to_tensor = T.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54ccb3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run on a folder of test images\n",
    "img_paths = sorted([p for p in test_images_dir.glob(\"*\") if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}])\n",
    "print(f\"Found {len(img_paths)} test images.\")\n",
    "\n",
    "for p in img_paths:\n",
    "    try:\n",
    "        im = Image.open(p).convert(\"RGB\")\n",
    "        det = infer_image(quant_model, im, score_thresh=score_thresh, max_dets=max_dets)\n",
    "        out_path = out_dir / p.name\n",
    "        visualize_and_save(im, det, out_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {p}: {e}\")\n",
    "\n",
    "print(f\"Done. Visualized detections saved to: {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912a37d",
   "metadata": {},
   "source": [
    "# Quantized float16 \n",
    "not scripted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95f8377",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "save_dir = Path(\"../models/frcnn_mobilenet_fp16\")  # <- where you saved your FP16-scripted model (if you did)\n",
    "# scripted_path = save_dir / \"model_scripted_fp16.pt\"\n",
    "model_path = save_dir / \"model_fp16_state_dict.pth\"  \n",
    "test_images_dir = Path(\"../data/test_images\")          # <-- change to your test folder\n",
    "out_dir = Path(\"../outputs/fp16_infer\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device: prefer CUDA for FP16; CPU stays float32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[FP16] Using device: {device}\")\n",
    "\n",
    "# Run on a folder of test images\n",
    "score_thresh = 0.5   # tweak as needed\n",
    "max_dets     = None  # e.g., 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c948d08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "model.to(device).eval()\n",
    "\n",
    "print(f\"Loaded model: {model_path.name}\")\n",
    "\n",
    "# Preprocessing: Faster R-CNN expects a FloatTensor [0,1], CxHxW\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "# Optional: class map (index -> name). Edit to your dataset.\n",
    "id2name = {\n",
    "    1: \"object\",  # <-- replace/add your class names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fdfa3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run on a folder of test images\n",
    "img_paths = sorted([p for p in test_images_dir.glob(\"*\") if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}])\n",
    "print(f\"Found {len(img_paths)} test images.\")\n",
    "\n",
    "for p in img_paths:\n",
    "    try:\n",
    "        im = Image.open(p).convert(\"RGB\")\n",
    "        det = infer_fp16_image(model, im, score_thresh=score_thresh, max_dets=max_dets)\n",
    "        out_path = out_dir / p.name\n",
    "        visualize_and_save(im, det, out_path, id2name=id2name)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {p}: {e}\")\n",
    "\n",
    "print(f\"Done. Visualized detections saved to: {out_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d5a0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b33c8d93",
   "metadata": {},
   "source": [
    "# VIDEO Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33a2fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_map = {1: \"object\"}  # your label map\n",
    "num_classes = 2  # 1 class + background; adjust to your training setup\n",
    "\n",
    "model_dir = \"distilled\" # \"frcnn_mobilenet_inter\"\n",
    "model_name = \"kd_resize_rot_frcnn_mobilenet_epoch8.pth\"\n",
    "model_path = Path(f\"../models/{model_dir}/{model_name}\")\n",
    "\n",
    "video_name = \"100-crop\"\n",
    "model_type = \"KD\"\n",
    "output_video_name = f\"output_{video_name}_{model_type}model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6a1c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#### mobilenet_v3_large_fpn #####\n",
    "print(f\"\\033[91m{model_path}\\033[0m\")\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "state = torch.load(model_path, weights_only=False, map_location=\"cpu\")  # load to CPU first (safer)\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# ##### resnet50_fpn_v2 backbone #####\n",
    "# print(f\"\\033[91m{model_path}\\033[0m\")\n",
    "# model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "# # get number of input features for the classifier\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# # replace the pre-trained head with a new one\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "# model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60066222",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- video I/O ----\n",
    "video_path = f\"../videos/{video_name}.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), f\"Failed to open {video_path}\"\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_video_path = f\"../videos/{output_video_name}.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "video_writer_out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "fps_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6e0b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- main loop ----\n",
    "while True:\n",
    "    start = time.time()\n",
    "    success, frame_bgr = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # OpenCV -> PIL (RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # run detection + visualization (returns PIL)\n",
    "    frame_vis_pil = visualize_detections(img_pil, model, class_map)\n",
    "\n",
    "    # PIL -> OpenCV (BGR)\n",
    "    frame_vis = cv2.cvtColor(np.array(frame_vis_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # keep original size if needed\n",
    "    if (frame_vis.shape[1], frame_vis.shape[0]) != (width, height):\n",
    "        frame_vis = cv2.resize(frame_vis, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # FPS\n",
    "    elapsed = time.time() - start\n",
    "    fps_list.append(1.0 / max(elapsed, 1e-6))\n",
    "    if len(fps_list) > 30:\n",
    "        fps_list.pop(0)\n",
    "    running_fps = float(np.mean(fps_list))\n",
    "\n",
    "    # overlay stats\n",
    "    frame_vis = display_text_block(frame_vis, [f\"FPS : {running_fps:.2f}\"])\n",
    "\n",
    "    # write\n",
    "    video_writer_out.write(frame_vis)\n",
    "\n",
    "print(f\"Mean FPS over last window: {running_fps:.2f}\")\n",
    "cap.release()\n",
    "video_writer_out.release()\n",
    "print(f\"Output video saved at {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a8eb1",
   "metadata": {},
   "source": [
    "# Compare videos \n",
    "Top-Bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e096137e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "play_top_bottom(\n",
    "    \"../videos/output_100-crop_KDmodel.avi\",\n",
    "    \"../videos/output_100-crop_Prunedmodel.avi\",\n",
    "    save_path=\"../videos/output_100-crop_KD_vs_Pruned_topbottom.avi\",\n",
    "    codec=\"XVID\",\n",
    "    display=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8addaab6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "play_side_by_side(\n",
    "    \"../videos/output_100-crop_KDmodel.avi\",\n",
    "    \"../videos/output_100-crop_Prunedmodel.avi\",\n",
    "    save_path=\"../videos/output_100-crop_KD vs Pruned.avi\",\n",
    "    codec=\"XVID\",\n",
    "    display=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb2567",
   "metadata": {},
   "source": [
    "# VIDEO Inference - Quantized float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d391ca8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_map = {1: \"object\"}  # your label map\n",
    "num_classes = 2  # 1 class + background; adjust to your training setup\n",
    "\n",
    "model_dir = \"frcnn_mobilenet_fp16\" # \"frcnn_mobilenet_inter\"\n",
    "model_name = \"model_fp16_state_dict.pth\"\n",
    "model_path = Path(f\"../models/{model_dir}/{model_name}\")\n",
    "\n",
    "video_name = \"100-crop\"\n",
    "output_video_name = f\"output_{video_name}_fl16_quant_model\"\n",
    "\n",
    "\n",
    "# Device: prefer CUDA for FP16; CPU stays float32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[FP16] Using device: {device}\")\n",
    "\n",
    "score_thresh = 0.5   # tweak as needed\n",
    "max_dets     = None  # e.g., 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935a3db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "in_feat = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_feat, num_classes)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Preprocessing: Faster R-CNN expects a FloatTensor [0,1], CxHxW\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "# Optional: class map (index -> name). Edit to your dataset.\n",
    "id2name = {\n",
    "    1: \"object\",  # <-- replace/add your class names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bdd1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- video I/O ----\n",
    "video_path = f\"../videos/{video_name}.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), f\"Failed to open {video_path}\"\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_video_path = f\"../videos/{output_video_name}.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "video_writer_out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "fps_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470b053",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- main loop ----\n",
    "while True:\n",
    "    start = time.time()\n",
    "    success, frame_bgr = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # OpenCV -> PIL (RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "    det = infer_image(model, img_pil, score_thresh=score_thresh, max_dets=max_dets)\n",
    "    # run detection + visualization (returns PIL)\n",
    "    frame_vis_pil = visualize_fp16(img_pil, det, class_map)\n",
    "\n",
    "    # PIL -> OpenCV (BGR)\n",
    "    frame_vis = cv2.cvtColor(np.array(frame_vis_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # keep original size if needed\n",
    "    if (frame_vis.shape[1], frame_vis.shape[0]) != (width, height):\n",
    "        frame_vis = cv2.resize(frame_vis, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # FPS\n",
    "    elapsed = time.time() - start\n",
    "    fps_list.append(1.0 / max(elapsed, 1e-6))\n",
    "    if len(fps_list) > 30:\n",
    "        fps_list.pop(0)\n",
    "    running_fps = float(np.mean(fps_list))\n",
    "\n",
    "    # overlay stats\n",
    "    frame_vis = display_text_block(frame_vis, [f\"FPS : {running_fps:.2f}\"])\n",
    "\n",
    "    # write\n",
    "    video_writer_out.write(frame_vis)\n",
    "\n",
    "print(f\"Mean FPS over last window: {running_fps:.2f}\")\n",
    "cap.release()\n",
    "video_writer_out.release()\n",
    "print(f\"Output video saved at {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43483ef5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc09fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d81b12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c6e68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fb326",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045c068",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e977b25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb9ee1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
