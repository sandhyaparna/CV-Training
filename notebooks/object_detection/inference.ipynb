{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31928d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ee8b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MODELS - https://github.com/pytorch/vision/tree/main/torchvision/models/detection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    ssd300_vgg16, SSD300_VGG16_Weights,\n",
    "    ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81918c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from model_inference import compare_two_models, visualize_detections, display_text_block\n",
    "from compare_videos import play_top_bottom, play_side_by_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6284d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- device ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def visualize_detections(img_pil, model, class_names, score_threshold=0.5, box_width=4):\n",
    "    \"\"\"\n",
    "    img_pil: PIL.Image (RGB)\n",
    "    class_names: dict mapping class_id -> string, e.g. {1: 'chuck'}\n",
    "    returns: PIL.Image with boxes drawn\n",
    "    \"\"\"\n",
    "    transform = transforms.ToTensor()\n",
    "    img_tensor = transform(img_pil).to(device)  # [C,H,W] on same device as model\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model([img_tensor])  # list of tensors\n",
    "\n",
    "    result = outputs[0]\n",
    "    scores = result[\"scores\"]\n",
    "    keep = scores > score_threshold\n",
    "    \n",
    "    boxes = result[\"boxes\"][keep].detach().to(\"cpu\")\n",
    "    labels = result[\"labels\"][keep].to(\"cpu\")\n",
    "    kept_scores = scores[keep].detach().to(\"cpu\")\n",
    "\n",
    "    # prepare label strings\n",
    "    label_names_raw = [class_names.get(lbl.item(), str(lbl.item())) for lbl in labels]\n",
    "    label_names = [f\"{name} - {score:.2f}\" for name, score in zip(label_names_raw, kept_scores)]\n",
    "\n",
    "    # draw on a CPU tensor version of the image (uint8 expected)\n",
    "    img_cpu = (img_tensor.to(\"cpu\") * 255).byte()\n",
    "    # warnings.warn(\"Argument 'font_size' will be ignored since 'font' is not set.\")\n",
    "    img_vis = draw_bounding_boxes(\n",
    "        img_cpu, boxes, labels=label_names, width=box_width, colors=\"black\"\n",
    "    )\n",
    "\n",
    "    return torchvision.transforms.ToPILImage()(img_vis)\n",
    "\n",
    "\n",
    "# # OPTIONAL: stub for display_text_block if you don't already have it\n",
    "def display_text_block(\n",
    "    frame_bgr,\n",
    "    lines,\n",
    "    x_start=10,\n",
    "    y_start=None,\n",
    "    x_gap=0,\n",
    "    y_gap=20,\n",
    "    text_color=(255, 255, 255),\n",
    "    background_color=(0, 0, 0),\n",
    "):\n",
    "    # Get frame dimensions\n",
    "    height, width = frame_bgr.shape[:2]\n",
    "\n",
    "    # If y_start is not provided, default to near bottom\n",
    "    if y_start is None:\n",
    "        y_start = height - 90\n",
    "\n",
    "    overlay = frame_bgr.copy()\n",
    "    x, y = x_start, y_start\n",
    "\n",
    "    for line in lines:\n",
    "        (tw, th), _ = cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        cv2.rectangle(\n",
    "            overlay, (x - 5, y - th - 5), (x + tw + 5, y + 5), background_color, -1\n",
    "        )\n",
    "        cv2.putText(\n",
    "            overlay,\n",
    "            line,\n",
    "            (x, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            text_color,\n",
    "            2,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "        x += x_gap\n",
    "        y += y_gap\n",
    "\n",
    "    alpha = 0.6\n",
    "    return cv2.addWeighted(overlay, alpha, frame_bgr, 1 - alpha, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b3f009",
   "metadata": {},
   "source": [
    "# 1 video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7b553",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_map = {1: \"object\"}  # your label map\n",
    "num_classes = 2  # 1 class + background; adjust to your training setup\n",
    "\n",
    "model_dir = \"../models\" # \"frcnn_mobilenet_inter\"\n",
    "model_type = \"v1\"\n",
    "model_name = f\"ftmo_{model_type}.pth\"\n",
    "model_path = Path(f\"{model_dir}/{model_name}\")\n",
    "\n",
    "input_video_dir = Path(f\"../videos_plant_not_working/raw\")\n",
    "output_video_dir = Path(f\"../videos_plant_not_working/{model_type}\")\n",
    "\n",
    "video_name = \"111\"\n",
    "output_video_name = f\"{video_name}_{model_type}\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e5f4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### resnet50_fpn_v2 backbone #####\n",
    "print(f\"\\033[91m{model_path}\\033[0m\")\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa89e0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- video I/O ----\n",
    "video_path = input_video_dir/f\"{video_name}.avi\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), f\"Failed to open {video_path}\"\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_video_path = output_video_dir/f\"{output_video_name}.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "video_writer_out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "fps_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a5a01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ---- main loop ----\n",
    "while True:\n",
    "    start = time.time()\n",
    "    success, frame_bgr = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # OpenCV -> PIL (RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # run detection + visualization (returns PIL)\n",
    "    frame_vis_pil = visualize_detections(img_pil, model, class_map)\n",
    "\n",
    "    # PIL -> OpenCV (BGR)\n",
    "    frame_vis = cv2.cvtColor(np.array(frame_vis_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # keep original size if needed\n",
    "    if (frame_vis.shape[1], frame_vis.shape[0]) != (width, height):\n",
    "        frame_vis = cv2.resize(frame_vis, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # FPS\n",
    "    elapsed = time.time() - start\n",
    "    fps_list.append(1.0 / max(elapsed, 1e-6))\n",
    "    if len(fps_list) > 30:\n",
    "        fps_list.pop(0)\n",
    "    running_fps = float(np.mean(fps_list))\n",
    "\n",
    "    # overlay stats\n",
    "    frame_vis = display_text_block(frame_vis, [f\"FPS : {running_fps:.2f}\"])\n",
    "\n",
    "    # write\n",
    "    video_writer_out.write(frame_vis)\n",
    "\n",
    "print(f\"Mean FPS over last window: {running_fps:.2f}\")\n",
    "cap.release()\n",
    "video_writer_out.release()\n",
    "print(f\"Output video saved at {output_video_path}\")\n",
    "# Mean FPS over last window: 6.37\n",
    "# Output video saved at ../videos_plant_not_working/v1/111_v1.avi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eaff93",
   "metadata": {},
   "source": [
    "# Multiple videos\n",
    "Modify model_type to v1, v2, v2_kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0915be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class_map = {1: \"chuck\"}  # your label map\n",
    "num_classes = 2  # 1 class + background; adjust to your training setup\n",
    "\n",
    "model_dir = \"../models\" # \"frcnn_mobilenet_inter\"\n",
    "model_type = \"v2\"\n",
    "model_name = f\"ftmo_{model_type}.pth\"\n",
    "model_path = Path(f\"{model_dir}/{model_name}\")\n",
    "\n",
    "input_video_dir = Path(f\"../videos_plant_not_working/raw\")\n",
    "output_video_dir = Path(f\"../videos_plant_not_working/{model_type}\")\n",
    "\n",
    "raw_videos = os.listdir(input_video_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e655d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "##### resnet50_fpn_v2 backbone #####\n",
    "print(f\"\\033[91m{model_path}\\033[0m\")\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False,  map_location=torch.device('cpu')))\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca83cbac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for raw_video in raw_videos:\n",
    "    # ---- video I/O ----\n",
    "    video_path = input_video_dir/f\"{raw_video}\"\n",
    "    print(video_path)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    assert cap.isOpened(), f\"Failed to open {video_path}\"\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    output_video_path = output_video_dir/f\"{raw_video}_{model_type}\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "    video_writer_out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    fps_list = []\n",
    "\n",
    "    # ---- main loop ----\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        success, frame_bgr = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "    \n",
    "        # OpenCV -> PIL (RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "        # run detection + visualization (returns PIL)\n",
    "        frame_vis_pil = visualize_detections(img_pil, model, class_map)\n",
    "    \n",
    "        # PIL -> OpenCV (BGR)\n",
    "        frame_vis = cv2.cvtColor(np.array(frame_vis_pil), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "        # keep original size if needed\n",
    "        if (frame_vis.shape[1], frame_vis.shape[0]) != (width, height):\n",
    "            frame_vis = cv2.resize(frame_vis, (width, height), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "        # FPS\n",
    "        elapsed = time.time() - start\n",
    "        fps_list.append(1.0 / max(elapsed, 1e-6))\n",
    "        if len(fps_list) > 30:\n",
    "            fps_list.pop(0)\n",
    "        running_fps = float(np.mean(fps_list))\n",
    "    \n",
    "        # overlay stats\n",
    "        frame_vis = display_text_block(frame_vis, [f\"FPS : {running_fps:.2f}\"])\n",
    "    \n",
    "        # write\n",
    "        video_writer_out.write(frame_vis)\n",
    "    \n",
    "    print(f\"Mean FPS over last window: {running_fps:.2f}\")\n",
    "    cap.release()\n",
    "    video_writer_out.release()\n",
    "    print(f\"Output video saved at {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1931d2b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
