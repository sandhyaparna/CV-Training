{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ed53f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install pycocotools\n",
    "!pip -q install albumentations\n",
    "!pip -q install torch torchvision\n",
    "!pip -q install matplotlib seaborn imutils opencv-contrib-python scikit-learn\n",
    "!pip -q install pandas mapcalc boto3\n",
    "!sudo apt-get update && sudo apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7639a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"../src\", \"../../src\")\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import json\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "from pathlib import Path\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44ac80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from data_loader import CustomDataset, ResizeTransform, ResizeRotateTransform, ResizeColorTransform, ResizeHorzTransform\n",
    "from engine import train_one_epoch, evaluate\n",
    "from utils import Tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8196d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MODELS - https://github.com/pytorch/vision/tree/main/torchvision/models/detection\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_320_fpn, FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    ssd300_vgg16, SSD300_VGG16_Weights,\n",
    "    ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    retinanet_resnet50_fpn_v2, RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fcos_resnet50_fpn, FCOS_ResNet50_FPN_Weights\n",
    ")\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "from torchvision.models.detection.fcos import FCOSHead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d985fb",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15b79e",
   "metadata": {},
   "source": [
    "### Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bdffe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/images_v1_v2'\n",
    "coco_path = '../data/annotations_v1_v2/coco_v1_v2.json'\n",
    " \n",
    "# 20% annotations\n",
    "aug_perc = 0.2\n",
    "sample_coco_path = f\"../data/annotations_v1_v2/coco_v1_v2_{aug_perc}.json\"\n",
    "\n",
    "image_size= (256, 256)  # (128, 128) (256, 256) (512, 512)\n",
    "batch_size = 2\n",
    "val_percent = 0.1\n",
    "num_classes =  2  # object + background class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f1142",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a31259",
   "metadata": {},
   "source": [
    "create dataset from image and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e79d7f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# create Dataset\n",
    "applied_transforms = ResizeTransform(size=image_size)\n",
    "\n",
    "dataset = CustomDataset(root=data_dir,\n",
    "                          annotation=coco_path,\n",
    "                          transforms=applied_transforms)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c1f24",
   "metadata": {},
   "source": [
    "Split data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755b171",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# set seed to split data into train & validation\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Number of data points in train & val\n",
    "val_size = int(val_percent * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "print(train_size, val_size)\n",
    "\n",
    "# Split data into train & val\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size], generator=generator1)\n",
    "print(len(train_ds), len(val_ds))\n",
    "print(\"shape of an image in the dataset:\", val_ds[0][0].shape)\n",
    "print(\"sample image & annotation bbox:\", val_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac813e",
   "metadata": {},
   "source": [
    "### Apply Augmnetations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac7372",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "aug_transforms = ResizeRotateTransform(size=image_size)\n",
    "\n",
    "aug_dataset = CustomDataset(root=data_dir,\n",
    "                          annotation=sample_coco_path,\n",
    "                          transforms=aug_transforms)\n",
    "\n",
    "# Concat train and augmenented data\n",
    "train_ds = torch.utils.data.ConcatDataset([train_ds, aug_dataset])\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe317df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "aug_transforms = ResizeColorTransform(size=image_size)\n",
    "\n",
    "aug_dataset = CustomDataset(root=data_dir,\n",
    "                          annotation=sample_coco_path,\n",
    "                          transforms=aug_transforms)\n",
    "\n",
    "# Concat train and augmenented data\n",
    "train_ds = torch.utils.data.ConcatDataset([train_ds, aug_dataset])\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d41b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "trns = \"resize_horz_clrjtr_rot\"\n",
    "aug_transforms = ResizeHorzTransform(size=image_size)\n",
    "\n",
    "aug_dataset = CustomDataset(root=data_dir,\n",
    "                          annotation=sample_coco_path,\n",
    "                          transforms=aug_transforms)\n",
    "\n",
    "# Concat train and augmenented data\n",
    "train_ds = torch.utils.data.ConcatDataset([train_ds, aug_dataset])\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c9d05",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac37200",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train DataLoader\n",
    "data_loader_train = torch.utils.data.DataLoader(train_ds,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "\n",
    "# Val DataLoader\n",
    "data_loader_val = torch.utils.data.DataLoader(val_ds,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e4c95",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2622bf",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "* * * * *\n",
    "\n",
    "üß† 1. Training hyperparameters\n",
    "------------------------------\n",
    "\n",
    "| Parameter | Meaning | Typical effect |\n",
    "| --- | --- | --- |\n",
    "| **`num_epochs`** | Number of full passes over the training dataset. | More epochs ‚Üí better convergence up to a point; too many ‚Üí overfitting. |\n",
    "| **`learning_rate`** | Step size for gradient updates. | Too high ‚Üí unstable or diverges; too low ‚Üí slow learning or stuck in local minima. |\n",
    "| **`momentum`** | Controls how much of the previous gradient is added to the current one (acts like inertia). | Helps smooth noisy gradients and escape shallow minima. Typical: 0.9--0.95. |\n",
    "| **`weight_decay`** | L2 regularization term. Penalizes large weights. | Helps reduce overfitting. Too high ‚Üí underfits. |\n",
    "| **`lr_scheduler`** | Reduces the learning rate over time. | Prevents overshooting late in training and helps refine minima. |\n",
    "| **`StepLR(step_size=3, gamma=0.1)`** | Every 3 epochs, multiply LR by 0.1. | Makes big drops in LR; better for short, sharp fine-tunes than long runs. |\n",
    "\n",
    "* * * * *\n",
    "\n",
    "üß© 2. Model structure parameters\n",
    "--------------------------------\n",
    "\n",
    "### (a) **ROI Heads** (Region of Interest heads)\n",
    "\n",
    "These control how the detector classifies and refines proposals after the RPN.\n",
    "\n",
    "| Parameter | Description | Effect / trade-off |\n",
    "| --- | --- | --- |\n",
    "| **`box_fg_iou_thresh`** | IoU threshold above which a proposal is labeled **foreground** (positive). | Higher ‚Üí fewer, cleaner positives. Lower ‚Üí more positives but noisier. |\n",
    "| **`box_bg_iou_thresh`** | IoU threshold below which a proposal is labeled **background** (negative). | Lower ‚Üí more negatives; higher ‚Üí fewer but \"harder\" negatives. |\n",
    "| **`positive_fraction`** | Fraction of samples per mini-batch that are positives. | Smaller ‚Üí model sees more background, good when background is confusing. |\n",
    "| **`batch_size_per_image`** | Number of sampled RoIs per image for training. | More samples ‚Üí better statistics but higher memory use. |\n",
    "| **`score_thresh`** | Minimum confidence score to keep a detection at inference. | Lower ‚Üí more detections (higher recall, lower precision). Higher ‚Üí fewer false positives but may miss objects. |\n",
    "| **`nms_thresh`** *(often added)* | IoU threshold for Non-Maximum Suppression between overlapping detections. | Lower ‚Üí fewer overlapping boxes (more aggressive suppression). Higher ‚Üí more duplicates. |\n",
    "| **`detections_per_img`** | Max number of detections returned per image. | Prevents cluttered outputs. |\n",
    "\n",
    "* * * * *\n",
    "\n",
    "### (b) **RPN (Region Proposal Network)**\n",
    "\n",
    "The RPN generates candidate regions likely to contain objects.\n",
    "\n",
    "| Parameter | Description | Effect / trade-off |\n",
    "| --- | --- | --- |\n",
    "| **`foreground_iou_thresh`** | IoU ‚â• this ‚Üí positive anchor. | Higher ‚Üí cleaner positives, fewer of them. |\n",
    "| **`background_iou_thresh`** | IoU ‚â§ this ‚Üí negative anchor. | Lower ‚Üí more negatives, helps discriminate similar background. |\n",
    "| **`pre_nms_top_n_train` / `pre_nms_top_n_test`** | Number of top-scoring anchors kept **before** NMS during training / testing. | Larger ‚Üí more proposals (higher recall, slower). |\n",
    "| **`post_nms_top_n_train` / `post_nms_top_n_test`** | Number of proposals kept **after** NMS. | Limits proposals sent to ROI head; smaller ‚Üí faster, possibly lower recall. |\n",
    "| **`nms_thresh`** | IoU threshold for NMS in the RPN. | Lower ‚Üí fewer overlapping proposals; higher ‚Üí more redundancy. |\n",
    "\n",
    "* * * * *\n",
    "\n",
    "‚öôÔ∏è 3. Optimizer & scheduler internals\n",
    "-------------------------------------\n",
    "\n",
    "| Parameter | Description | Why it matters |\n",
    "| --- | --- | --- |\n",
    "| **`optimizer = torch.optim.SGD(...)`** | Stochastic Gradient Descent updates weights each mini-batch. | Standard choice for detection; stable and efficient. |\n",
    "| **`params = [p for p in model.parameters() if p.requires_grad]`** | Only train unfrozen layers. | Useful if you freeze backbone for fine-tuning. |\n",
    "| **`gamma` in scheduler** | Multiplicative LR decay factor. | 0.1 ‚Üí LR drops to 10% at each step. |\n",
    "| **`step_size`** | How often (in epochs) to decay LR. | Smaller ‚Üí more frequent drops; larger ‚Üí smoother. |\n",
    "\n",
    "* * * * *\n",
    "\n",
    "üß† 4. Conceptual relationships\n",
    "------------------------------\n",
    "\n",
    "-   **IoU thresholds** decide what counts as positive/negative. Setting the gap between foreground and background helps avoid ambiguous samples.\n",
    "\n",
    "-   **Positive fraction** and **batch_size_per_image** together define the ratio of object vs. background regions the model learns from.\n",
    "\n",
    "-   **RPN thresholds** affect *proposal quality* and recall; **ROI head thresholds** affect *classification precision*.\n",
    "\n",
    "-   **Score & NMS thresholds** matter mostly at inference --- they balance precision vs. recall.\n",
    "\n",
    "-   **Learning rate & scheduler** govern how fast and steadily weights adapt.\n",
    "\n",
    "-   **Momentum** and **weight decay** keep optimization stable and generalizable.\n",
    "\n",
    "* * * * *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85804b",
   "metadata": {},
   "source": [
    "### Modeling Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d2aff5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "model_type = Path(\"../models/fasterrcnn_resnet50_fpn_v2\")\n",
    "\n",
    "log_dir = Path(model_type)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_path = log_dir / f\"training_log.txt\"\n",
    "\n",
    "num_epochs = 25\n",
    "# Rule of thumb: ~0.0025 per image. So LR ‚âà 0.0025 √ó (total_batch). Examples: total_batch=8 ‚Üí 0.02; total_batch=4 ‚Üí 0.01; total_batch=2 ‚Üí 0.005. Try a sweep: {0.005, 0.01, 0.02}.\n",
    "learning_rate = 0.0005   # {0.005, 0.01, 0.02}\n",
    "momentum = 0.9 # {0.9-0.95}\n",
    "weight_decay=0.0001   # {0.0001, 0.0005}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d907b76",
   "metadata": {},
   "source": [
    "### Base Model: Detection Framework + Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf217ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1)\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# --- ROI Heads: emphasize hard negatives, be stricter at test time ---\n",
    "model.roi_heads.box_fg_iou_thresh = 0.5  # Increase positive samples - 0.8\n",
    "model.roi_heads.box_bg_iou_thresh = 0.3  # Lower to include harder negatives - 0.2\n",
    "model.roi_heads.positive_fraction = 0.35   # Loosen foreground criteria - 0.25\n",
    "model.roi_heads.batch_size_per_image = 512\n",
    "\n",
    "# Make RPN positives cleaner and fewer noisy proposals\n",
    "model.rpn.foreground_iou_thresh = 0.8          # default 0.7\n",
    "model.rpn.background_iou_thresh = 0.2          # default 0.3\n",
    "model.rpn.pre_nms_top_n_train  = 1200          # default 2000\n",
    "model.rpn.post_nms_top_n_train = 512           # default 1000\n",
    "model.rpn.pre_nms_top_n_test   = 600           # default 1000\n",
    "model.rpn.post_nms_top_n_test  = 300           # default 1000\n",
    "model.rpn.nms_thresh = 0.6                     # default 0.7\n",
    "\n",
    "# Inference thresholds (tune via PR curve)\n",
    "model.roi_heads.nms_thresh = 0.5\n",
    "model.roi_heads.score_thresh = 0.3   # Allow lower-confidence detections - 0.5\n",
    "model.roi_heads.detections_per_img = 10\n",
    "\n",
    "# move model to the current device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# It returns all trainable parameters of the model\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "  optimizer,\n",
    "  step_size=3,\n",
    "  gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c454df",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91725996",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(log_path, \"w\") as f:\n",
    "    tee = Tee(sys.stdout, f)\n",
    "    with redirect_stdout(tee):\n",
    "        for epoch in range(num_epochs):\n",
    "            # train one epoch\n",
    "            train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=100)\n",
    "\n",
    "            # update LR\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            evaluate(model, data_loader_val, device=device)\n",
    "\n",
    "            # save model weights\n",
    "            model_name = f\"20_{trns}_epoch{epoch}\"\n",
    "            model_save_path = model_type / f\"{model_name}.pth\"\n",
    "            print(f\"\\nModel saved as: {model_save_path}\\n\")\n",
    "\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete. Logs saved to {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f47cdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e148142",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f2313",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79279f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
